{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import  pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16, DenseNet121, ResNet50, DenseNet201\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPool2D, BatchNormalization, Attention, Reshape, Multiply\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import load_model\n",
    "from dataclasses import dataclass, asdict\n",
    "import yaml\n",
    "from typing import Optional, Union, List\n",
    "from dacite import from_dict\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import itertools\n",
    "import ast\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking if GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda create -n seaborn r python=3.6\n",
    "activate seaborn\n",
    "pip install --ignore-installed --upgrade seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = '/home/ubuntu/data/3classes/HAM10000_by_class/'\n",
    "data_dir = '/home/ubuntu/data/HAM10000_all_images'\n",
    "df = pd.read_csv('/home/ubuntu/data/bkp/HAM10000_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'mel': 'mel', 'bcc':'bcc', 'nv':'others', 'bkl':'others', 'akiec':'others','vasc':'others','df':'others'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nv       6705\n",
       "mel      1113\n",
       "bkl      1099\n",
       "bcc       514\n",
       "akiec     327\n",
       "vasc      142\n",
       "df        115\n",
       "Name: dx, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dx\"].replace(mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "others    8388\n",
       "mel       1113\n",
       "bcc        514\n",
       "Name: dx, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['others', 'mel', 'bcc']\n"
     ]
    }
   ],
   "source": [
    "label = df['dx'].unique().tolist()\n",
    "print(label)\n",
    "label_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/3classes/HAM10000_by_class/others/\n",
      "/home/ubuntu/data/3classes/HAM10000_by_class/mel/\n",
      "/home/ubuntu/data/3classes/HAM10000_by_class/bcc/\n"
     ]
    }
   ],
   "source": [
    "for i in label:\n",
    "    print(dest_dir + str(i) + '/')\n",
    "    try:\n",
    "        os.mkdir(dest_dir + str(i))\n",
    "    except OSError:\n",
    "        pass\n",
    "#     os.mkdir(dest_dir + str(i))\n",
    "    sample = df[df['dx'] == i]['image_id']\n",
    "    label_images.extend(sample)\n",
    "    for id in label_images:\n",
    "        shutil.copyfile((data_dir + '/' + id + '.jpg'), (dest_dir + str(i) + '/' + id + '.jpg'))\n",
    "    label_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmdir /home/ubuntu/data/3classes/HAM10000_by_class/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Split input images into train and test images based on the percentage passed as input\n",
    "'''\n",
    "import numpy as np\n",
    "def split_dataset_into_test_and_train_sets(all_data_dir, training_data_dir, testing_data_dir, testing_data_pct):\n",
    "    # Recreate testing and training directories\n",
    "    if testing_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(testing_data_dir, ignore_errors=False)\n",
    "        os.makedirs(testing_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + testing_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + testing_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "\n",
    "    if training_data_dir.count('/') > 1:\n",
    "        shutil.rmtree(training_data_dir, ignore_errors=False)\n",
    "        os.makedirs(training_data_dir)\n",
    "        print(\"Successfully cleaned directory \" + training_data_dir)\n",
    "    else:\n",
    "        print(\"Refusing to delete testing data directory \" + training_data_dir + \" as we prevent you from doing stupid things!\")\n",
    "\n",
    "    num_training_files = 0\n",
    "    num_testing_files = 0\n",
    "\n",
    "    for subdir, dirs, files in os.walk(all_data_dir):\n",
    "        category_name = os.path.basename(subdir)\n",
    "\n",
    "        # Don't create a subdirectory for the root directory\n",
    "        print(category_name + \" vs \" + os.path.basename(all_data_dir))\n",
    "        if category_name == os.path.basename(all_data_dir):\n",
    "            continue\n",
    "\n",
    "        training_data_category_dir = training_data_dir + '/' + category_name\n",
    "        testing_data_category_dir = testing_data_dir + '/' + category_name\n",
    "\n",
    "        if not os.path.exists(training_data_category_dir):\n",
    "            os.mkdir(training_data_category_dir)\n",
    "\n",
    "        if not os.path.exists(testing_data_category_dir):\n",
    "            os.mkdir(testing_data_category_dir)\n",
    "\n",
    "        for file in files:\n",
    "            input_file = os.path.join(subdir, file)\n",
    "            if np.random.rand(1) < testing_data_pct:\n",
    "                shutil.copy(input_file, testing_data_dir + '/' + category_name + '/' + file)\n",
    "                num_testing_files += 1\n",
    "            else:\n",
    "                shutil.copy(input_file, training_data_dir + '/' + category_name + '/' + file)\n",
    "                num_training_files += 1\n",
    "\n",
    "    print(\"Processed \" + str(num_training_files) + \" training files.\")\n",
    "    print(\"Processed \" + str(num_testing_files) + \" testing files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cleaned directory /home/ubuntu/data/3classes/HAM10000_test_by_class/\n",
      "Successfully cleaned directory /home/ubuntu/data/3classes/HAM10000_train_by_class/\n",
      " vs \n",
      "mel vs \n",
      "others vs \n",
      "bcc vs \n",
      "Processed 7964 training files.\n",
      "Processed 2051 testing files.\n"
     ]
    }
   ],
   "source": [
    "all_data_dir = '/home/ubuntu/data/3classes/HAM10000_by_class/'\n",
    "training_data_dir = '/home/ubuntu/data/3classes/HAM10000_train_by_class/'\n",
    "testing_data_dir = '/home/ubuntu/data/3classes/HAM10000_test_by_class/'\n",
    "\n",
    "testing_data_pct = 0.2\n",
    "split_dataset_into_test_and_train_sets(all_data_dir, training_data_dir, testing_data_dir, testing_data_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_augmentation:\n",
    "    def __init__(self, image):\n",
    "        '''\n",
    "        Import image\n",
    "        :param path: Path to the image\n",
    "        :param image_name: image name\n",
    "        '''\n",
    "        self.image = cv2.imread(image)\n",
    "        #print(image)\n",
    "\n",
    "    def rotate(self, image, angle=90, scale=1.0):\n",
    "        '''\n",
    "        Rotate the image\n",
    "        :param image: image to be processed\n",
    "        :param angle: Rotation angle in degrees. Positive values mean counter-clockwise rotation (the coordinate origin is assumed to be the top-left corner).\n",
    "        :param scale: Isotropic scale factor.\n",
    "        '''\n",
    "        w = image.shape[1]\n",
    "        h = image.shape[0]\n",
    "        #rotate matrix\n",
    "        M = cv2.getRotationMatrix2D((w/2,h/2), angle, scale)\n",
    "        #rotate\n",
    "        image = cv2.warpAffine(image,M,(w,h))\n",
    "        return image\n",
    "\n",
    "    def flip(self, image, vflip=False, hflip=False):\n",
    "        '''\n",
    "        Flip the image\n",
    "        :param image: image to be processed\n",
    "        :param vflip: whether to flip the image vertically\n",
    "        :param hflip: whether to flip the image horizontally\n",
    "        '''\n",
    "        if hflip or vflip:\n",
    "            if hflip and vflip:\n",
    "                c = -1\n",
    "            else:\n",
    "                c = 0 if vflip else 1\n",
    "            image = cv2.flip(image, flipCode=c)\n",
    "        return image \n",
    "    \n",
    "    \n",
    "    def image_augment(self): \n",
    "        '''\n",
    "        Create the new image with imge augmentation\n",
    "        :param path: the path to store the new image\n",
    "        ''' \n",
    "        rotate_range = [90,180]\n",
    "        vflip = bool(random.randrange(2))\n",
    "        hflip = bool(random.randrange(2))\n",
    "        \n",
    "        image = self.image\n",
    "        \n",
    "        image = self.rotate(image, rotate_range[random.randrange(len(rotate_range))])\n",
    "        image = self.flip(image, vflip, hflip)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel 8500 0\n",
      "others 8500 0\n",
      "bcc 5208 3292\n"
     ]
    }
   ],
   "source": [
    "#upsampling\n",
    "\n",
    "training_data_dir = '/home/ubuntu/data/3classes/HAM10000_train_by_class/'\n",
    "aug_size = 8500\n",
    "rand_images = {}\n",
    "for dir in os.listdir(training_data_dir):\n",
    "    images = os.listdir(f\"{training_data_dir}/{dir}\")\n",
    "    images_count = len(images)\n",
    "    to_balance = aug_size - images_count\n",
    "    print(dir, images_count, to_balance)\n",
    "    rand_images[dir] = list()\n",
    "    for i in range(to_balance):\n",
    "        rand = randrange(images_count - 1)\n",
    "        rand_images[dir].append(rand)\n",
    "        img_path = f\"{training_data_dir}/{dir}/{images[rand]}\"\n",
    "        img_aug = Data_augmentation(img_path).image_augment()\n",
    "        cv2.imwrite(f\"{img_path.split('.')[0]}_aug{i}.jpg\", img_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objects and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Resize:\n",
    "    resizeW: int\n",
    "    resizeH: int\n",
    "\n",
    "@dataclass\n",
    "class DullRazor:\n",
    "    enabled: bool\n",
    "    razorblur: str\n",
    "    mediankernel_razorblur: int\n",
    "    filterstructure:int\n",
    "    lowerbound:int\n",
    "    inpaintmat:int\n",
    "\n",
    "@dataclass\n",
    "class Blur:\n",
    "    enabled: bool\n",
    "    normalblur: str\n",
    "    mediankernel_blur: int\n",
    "    blurnum: int\n",
    "\n",
    "@dataclass\n",
    "class Softattention:\n",
    "    alpha: float\n",
    "    beta: float\n",
    "    gamma: float\n",
    "    \n",
    "@dataclass\n",
    "class AttentionConfig:\n",
    "    resize: Resize\n",
    "    dull_razor: DullRazor\n",
    "    blur: Blur\n",
    "    soft_attention: Softattention\n",
    "        \n",
    "@dataclass\n",
    "class LossParams:\n",
    "    func: str\n",
    "    params: Optional[str]\n",
    "\n",
    "@dataclass\n",
    "class OptimizerParams:\n",
    "    func: str\n",
    "    params: Optional[str]\n",
    "\n",
    "@dataclass\n",
    "class InputDataParams:\n",
    "    input_size: str\n",
    "\n",
    "@dataclass\n",
    "class ModelParams:\n",
    "    batch_size: int\n",
    "    arch: str\n",
    "    freeze_pretrained: bool\n",
    "    steps_per_epoch: int\n",
    "    metrics: List[str]\n",
    "    pretrained_weight: Optional[str]\n",
    "    loss: Optional[LossParams]\n",
    "    optimizer: Optional[OptimizerParams]\n",
    "    class_weight_mu: float\n",
    "\n",
    "@dataclass\n",
    "class ModelsConfig:\n",
    "    model_name: str\n",
    "    input_params: InputDataParams\n",
    "    model_params: ModelParams\n",
    "    attention_config: Optional[AttentionConfig]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml for base and resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yaml for ResNet50\n",
    "yaml_config3 = \"\"\"\n",
    "ham_10000_resnet50:\n",
    "  model_name: \"HAM_10000_ResNet50\"\n",
    "  input_params:\n",
    "    input_size: (224,224)\n",
    "  model_params:\n",
    "    batch_size: 32\n",
    "    arch: tf.keras.applications.ResNet50\n",
    "    freeze_pretrained: True\n",
    "    steps_per_epoch: 100\n",
    "    metrics: ['accuracy']\n",
    "    loss:\n",
    "      func: \"sparse_categorical_crossentropy\"\n",
    "    optimizer:\n",
    "      func: \"Adam\"\n",
    "      params: \"{'learning_rate':1e-5}\"\n",
    "    class_weight_mu: 1\n",
    "  attention_config:\n",
    "    resize:\n",
    "      resizeW: 224\n",
    "      resizeH: 224\n",
    "    dull_razor:\n",
    "      enabled: True\n",
    "      razorblur: \"M\"\n",
    "      mediankernel_razorblur: 3\n",
    "      filterstructure: 5\n",
    "      lowerbound: 5\n",
    "      inpaintmat: 3\n",
    "    blur:\n",
    "      enabled: True\n",
    "      normalblur: \"M\"\n",
    "      mediankernel_blur: 5\n",
    "      blurnum: 5\n",
    "    soft_attention:\n",
    "      alpha: 0.7\n",
    "      beta: 0.3\n",
    "      gamma: 0.0\n",
    "      \n",
    "ham_10000_resnet50_3D&D600450:\n",
    "  model_name: \"ham_10000_resnet50_3D&D600450\"\n",
    "  input_params:\n",
    "    input_size: (600,450)\n",
    "  model_params:\n",
    "    batch_size: 32\n",
    "    arch: tf.keras.applications.ResNet50\n",
    "    freeze_pretrained: True\n",
    "    steps_per_epoch: 100\n",
    "    metrics: ['accuracy']\n",
    "    loss:\n",
    "      func: \"sparse_categorical_crossentropy\"\n",
    "    optimizer:\n",
    "      func: \"Adam\"\n",
    "      params: \"{'learning_rate':1e-4}\"\n",
    "    class_weight_mu: 1\n",
    "  attention_config:\n",
    "    resize:\n",
    "      resizeW: 224\n",
    "      resizeH: 224\n",
    "    dull_razor:\n",
    "      enabled: True\n",
    "      razorblur: \"M\"\n",
    "      mediankernel_razorblur: 3\n",
    "      filterstructure: 5\n",
    "      lowerbound: 5\n",
    "      inpaintmat: 3\n",
    "    blur:\n",
    "      enabled: True\n",
    "      normalblur: \"M\"\n",
    "      mediankernel_blur: 5\n",
    "      blurnum: 5\n",
    "    soft_attention:\n",
    "      alpha: 0.7\n",
    "      beta: 0.3\n",
    "      gamma: 0.0\n",
    "      \n",
    "ham_10k_resnet50_600450:\n",
    "  model_name: \"HAM_10k_ResNet50_600450\"\n",
    "  input_params:\n",
    "    input_size: (600,450)\n",
    "  model_params:\n",
    "    batch_size: 32\n",
    "    arch: tf.keras.applications.ResNet50\n",
    "    freeze_pretrained: True\n",
    "    steps_per_epoch: 100\n",
    "    metrics: ['accuracy']\n",
    "    loss:\n",
    "      func: \"sparse_categorical_crossentropy\"\n",
    "    optimizer:\n",
    "      func: \"Adam\"\n",
    "      params: \"{'learning_rate':1e-4}\"\n",
    "    class_weight_mu: 1\n",
    "  attention_config:\n",
    "    resize:\n",
    "      resizeW: 224\n",
    "      resizeH: 224\n",
    "    dull_razor:\n",
    "      enabled: True\n",
    "      razorblur: \"M\"\n",
    "      mediankernel_razorblur: 3\n",
    "      filterstructure: 5\n",
    "      lowerbound: 5\n",
    "      inpaintmat: 3\n",
    "    blur:\n",
    "      enabled: True\n",
    "      normalblur: \"M\"\n",
    "      mediankernel_blur: 5\n",
    "      blurnum: 5\n",
    "    soft_attention:\n",
    "      alpha: 0.7\n",
    "      beta: 0.3\n",
    "      gamma: 0.0\n",
    "      \n",
    "ham_10k_resnet50_512:\n",
    "  model_name: \"HAM_10k_ResNet50_512\"\n",
    "  input_params:\n",
    "    input_size: (512,512)\n",
    "  model_params:\n",
    "    batch_size: 32\n",
    "    arch: tf.keras.applications.ResNet50\n",
    "    freeze_pretrained: True\n",
    "    steps_per_epoch: 100\n",
    "    metrics: ['accuracy']\n",
    "    loss:\n",
    "      func: \"sparse_categorical_crossentropy\"\n",
    "    optimizer:\n",
    "      func: \"Adam\"\n",
    "      params: \"{'learning_rate':1e-4}\"\n",
    "    class_weight_mu: 1\n",
    "  attention_config:\n",
    "    resize:\n",
    "      resizeW: 224\n",
    "      resizeH: 224\n",
    "    dull_razor:\n",
    "      enabled: True\n",
    "      razorblur: \"M\"\n",
    "      mediankernel_razorblur: 3\n",
    "      filterstructure: 5\n",
    "      lowerbound: 5\n",
    "      inpaintmat: 3\n",
    "    blur:\n",
    "      enabled: True\n",
    "      normalblur: \"M\"\n",
    "      mediankernel_blur: 5\n",
    "      blurnum: 5\n",
    "    soft_attention:\n",
    "      alpha: 0.7\n",
    "      beta: 0.3\n",
    "      gamma: 0.0\n",
    "      \n",
    "ham_10000_resnet50_noextralayers:\n",
    "  model_name: \"ham_10000_resnet50_noextralayers\"\n",
    "  input_params:\n",
    "    input_size: (600,450)\n",
    "  model_params:\n",
    "    batch_size: 32\n",
    "    arch: tf.keras.applications.ResNet50\n",
    "    freeze_pretrained: True\n",
    "    steps_per_epoch: 100\n",
    "    metrics: ['accuracy']\n",
    "    loss:\n",
    "      func: \"sparse_categorical_crossentropy\"\n",
    "    optimizer:\n",
    "      func: \"Adam\"\n",
    "      params: \"{'learning_rate':1e-4}\"\n",
    "    class_weight_mu: 1\n",
    "  attention_config:\n",
    "    resize:\n",
    "      resizeW: 224\n",
    "      resizeH: 224\n",
    "    dull_razor:\n",
    "      enabled: True\n",
    "      razorblur: \"M\"\n",
    "      mediankernel_razorblur: 3\n",
    "      filterstructure: 5\n",
    "      lowerbound: 5\n",
    "      inpaintmat: 3\n",
    "    blur:\n",
    "      enabled: True\n",
    "      normalblur: \"M\"\n",
    "      mediankernel_blur: 5\n",
    "      blurnum: 5\n",
    "    soft_attention:\n",
    "      alpha: 0.7\n",
    "      beta: 0.3\n",
    "      gamma: 0.0\n",
    "      \n",
    "ham_10000_resnet50_4D&D:\n",
    "  model_name: \"ham_10000_resnet50_4D&D\"\n",
    "  input_params:\n",
    "    input_size: (600,450)\n",
    "  model_params:\n",
    "    batch_size: 32\n",
    "    arch: tf.keras.applications.ResNet50\n",
    "    freeze_pretrained: True\n",
    "    steps_per_epoch: 100\n",
    "    metrics: ['accuracy']\n",
    "    loss:\n",
    "      func: \"sparse_categorical_crossentropy\"\n",
    "    optimizer:\n",
    "      func: \"Adam\"\n",
    "      params: \"{'learning_rate':1e-4}\"\n",
    "    class_weight_mu: 1\n",
    "  attention_config:\n",
    "    resize:\n",
    "      resizeW: 224\n",
    "      resizeH: 224\n",
    "    dull_razor:\n",
    "      enabled: True\n",
    "      razorblur: \"M\"\n",
    "      mediankernel_razorblur: 3\n",
    "      filterstructure: 5\n",
    "      lowerbound: 5\n",
    "      inpaintmat: 3\n",
    "    blur:\n",
    "      enabled: True\n",
    "      normalblur: \"M\"\n",
    "      mediankernel_blur: 5\n",
    "      blurnum: 5\n",
    "    soft_attention:\n",
    "      alpha: 0.7\n",
    "      beta: 0.3\n",
    "      gamma: 0.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ham10000Attention:\n",
    "    \n",
    "    def __init__(self, model, attention_config):\n",
    "        self.config = attention_config\n",
    "        \n",
    "        self.input_shape = (self.config.resize.resizeW, self.config.resize.resizeH)\n",
    "        self.model = model\n",
    "        #self.attention_output = self.heatmap(self.image, model)\n",
    "    \n",
    "    def resize(self, img):\n",
    "        '''\n",
    "        Resizes the image\n",
    "        :param img: image to be processed\n",
    "        '''\n",
    "        return cv2.resize(img, self.input_shape, interpolation=cv2.INTER_LINEAR)    \n",
    "            \n",
    "    def dull_razor(self, img):\n",
    "        '''\n",
    "        Removes noises from the images(eg. Hair)\n",
    "        :param img: image to be processed\n",
    "        '''\n",
    "        cfg = self.config.dull_razor\n",
    "        if cfg.razorblur == \"M\":\n",
    "            img = cv2.medianBlur(img,cfg.mediankernel_razorblur)\n",
    "        elif cfg.razorblur == \"G\":\n",
    "            img = cv2.GaussianBlur(img, (cfg.mediankernel_razorblur, cfg.mediankernel_razorblur),0)\n",
    "\n",
    "        #gyimage = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        #filtersize = (cfg.filterstructure,cfg.filterstructure)\n",
    "        #kernelrazor = cv2.getStructuringElement(cv2.MORPH_RECT, filtersize)\n",
    "        #gyimage = cv2.morphologyEx(gyimage, cv2.MORPH_BLACKHAT, kernelrazor)\n",
    "#\n",
    "        #retrazor, maskrazor = cv2.threshold(gyimage, cfg.lowerbound, 255, cv2.THRESH_BINARY)\n",
    "        #img = cv2.inpaint(img, maskrazor, cfg.inpaintmat, cv2.INPAINT_TELEA)\n",
    "        return img\n",
    "\n",
    "    def blur(self, img):\n",
    "        '''\n",
    "        Applies blur to the image\n",
    "        :param img: image to be processed\n",
    "        '''\n",
    "        cfg = self.config.blur\n",
    "        if cfg.normalblur == \"M\":\n",
    "            img = cv2.medianBlur(img, cfg.mediankernel_blur)\n",
    "        elif cfg.normalblur == \"G\":\n",
    "            img = cv2.GaussianBlur(img, (cfg.mediankernel_blur, cfg.mediankernel_blur), 0)\n",
    "        return img\n",
    "\n",
    "    def softention_preprocess(self, img):\n",
    "        first = preprocess_input(img)\n",
    "        expanded_image = np.expand_dims(first, 0)\n",
    "        return expanded_image\n",
    "\n",
    "    def softention_mapping(self, img, LayerNumber, input_shape, SoftentionImage):\n",
    "        cfg = self.config.soft_attention\n",
    "        activated = self.model.predict(img)\n",
    "        output = np.abs(activated)\n",
    "        output = np.sum(output, axis = -1).squeeze() \n",
    "        output = cv2.resize(output, input_shape)\n",
    "        output /= output.max() \n",
    "        #output *= 255 \n",
    "        #Weights =  255 - output.astype('uint8')\n",
    "#\n",
    "        #heatmap = cv2.applyColorMap(Weights, cv2.COLORMAP_JET)\n",
    "        #heatmap = cv2.addWeighted(heatmap, cfg.alpha, SoftentionImage, cfg.beta, cfg.gamma)\n",
    "        return output\n",
    "    \n",
    "    def heatmap(self, img):\n",
    "        #resized_image = self.resize(img)\n",
    "        hair_removed_image = self.dull_razor(img)\n",
    "        softentionImage = self.blur(hair_removed_image)\n",
    "        expanded_image = self.softention_preprocess(softentionImage)\n",
    "        heatmap = self.softention_mapping(expanded_image, -1, self.input_shape, softentionImage)\n",
    "        return heatmap\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        '''\n",
    "        Applies soft attention segmentation to the image\n",
    "        :param img: image to be processed\n",
    "        '''        \n",
    "        img = self.resize(img)\n",
    "        heatmap = self.heatmap(img)\n",
    "        mask = heatmap.reshape(self.config.resize.resizeW,self.config.resize.resizeH,1)\n",
    "        out = Multiply()([tf.cast(img, tf.float32),mask])\n",
    "        img = tf.keras.utils.normalize(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer():\n",
    "    \n",
    "    def __init__(self, train_dir, test_dir, model_name, model_dir, batch_size = 32, \n",
    "               target_size = (600,450), model_params = None, class_weight_mu = 0.4, attention_config = None,retrain = False):\n",
    "        self.train_dir = train_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.model_name = model_name\n",
    "        self.model_dir = model_dir\n",
    "        self.checkpoint_path = f\"{self.model_dir}/{self.model_name}\"\n",
    "        if attention_config:\n",
    "            print(\"attention model\")\n",
    "            self.attention_model = self.attention(attention_config)\n",
    "        else:\n",
    "            self.attention_model = None\n",
    "        self.train_generator, self.validation_generator, self.attention_generator = self.get_generators()\n",
    "        self.set_class_weight(class_weight_mu)\n",
    "        self.model_params = model_params\n",
    "        if retrain == True or not(self.load_trained_model()):\n",
    "            if model_params:\n",
    "                self.register_model(self.model_architecture(model_params))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_config(config, base_dir = '/home/ubuntu/data/3classes', retrain = False):\n",
    "        '''\n",
    "        Factory method to create Modeltrainer instance from YAML config\n",
    "        :param config: Config file that contains model training configuration\n",
    "        :param base_dir: Base directory of the training process where training images and testing images are stored.\n",
    "        :param train_dir: Directory containing the training images. This should be under the base directory.\n",
    "        :param test_dir: Directory containing the testing images. This should be under the base directory.\n",
    "        :param retrain: Boolean parameter that tells whether a model has to be training from the beginning or an already trained model needs to be loaded from the base path\n",
    "        :param model_weights: Specifies from where the weights for a pretrained models needs to be fetched from.\n",
    "        '''\n",
    "        train_dir = f\"{base_dir}/HAM10000_train_by_class/\"\n",
    "        test_dir = f\"{base_dir}/HAM10000_test_by_class/\"\n",
    "        model_dir = f\"{base_dir}/models\"\n",
    "        cfg = from_dict(data_class=ModelsConfig, data=config)\n",
    "        model_trainer = ModelTrainer(train_dir, test_dir, cfg.model_name, model_dir, \n",
    "                                     batch_size = cfg.model_params.batch_size or 16,\n",
    "                                     target_size = eval(cfg.input_params.input_size),\n",
    "                                     class_weight_mu = cfg.model_params.class_weight_mu,\n",
    "                                     model_params = cfg.model_params, attention_config = cfg.attention_config, retrain = retrain)      \n",
    "        return model_trainer   \n",
    "\n",
    "    def attention(self, attention_config):\n",
    "        '''\n",
    "        Creates soft attention map\n",
    "        attention_config: Contains the parametes for the attention model.\n",
    "        '''\n",
    "        attention_pretrained_model = ResNet50(input_shape=(224,224,3),\n",
    "                                                    include_top=False,\n",
    "                                                    weights='imagenet')\n",
    "        out_layer = attention_pretrained_model.layers[-1]\n",
    "        model = tf.keras.models.Model(inputs = attention_pretrained_model.inputs, outputs = out_layer.output)\n",
    "        return Ham10000Attention(model,attention_config)\n",
    "\n",
    "    def register_model(self, model):\n",
    "        '''\n",
    "        Registers the model to be used while instantiating the model trainer object\n",
    "        :param model: Model that needs to be registered. It can either be a pretrained model or a custom model.\n",
    "        '''\n",
    "        self.model = model\n",
    "        self.model.compile(loss=self.model_params.loss.func, optimizer=self.optimizer(), metrics=self.model_params.metrics)\n",
    "        if not os.path.exists(self.checkpoint_path):\n",
    "            os.makedirs(self.checkpoint_path)\n",
    "        self.model.save(f\"{self.checkpoint_path}/model.h5\")\n",
    "\n",
    "    def optimizer(self):\n",
    "        '''\n",
    "        Configures the optimizer using the specifications from the confifuration file\n",
    "        '''\n",
    "        optimizer_func = eval(self.model_params.optimizer.func)\n",
    "        optimizer_params = ast.literal_eval(self.model_params.optimizer.params)\n",
    "        optimizer = optimizer_func(**optimizer_params)\n",
    "        return optimizer\n",
    "\n",
    "    def summary(self):\n",
    "        '''\n",
    "        Provides a summary of the model - list all the layers and the number of parameters in each layer\n",
    "        '''\n",
    "        self.model.summary()\n",
    "\n",
    "    def show_samples(self, rows = 4, columns = 4):\n",
    "        '''\n",
    "        Displays sample images\n",
    "        '''\n",
    "        x, y = next(self.train_generator)\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        for i in range(0, columns*rows):\n",
    "            img = x[i].astype(int)\n",
    "            fig.add_subplot(rows, columns, i+1)\n",
    "            plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "    def get_generators(self):\n",
    "        '''\n",
    "        Creates training, testing and validation data generators. These generators will be used to generate images in batches for\n",
    "        each step in training. Featurewise centering and normalisation is done as well.\n",
    "        '''\n",
    "        train_datagen = ImageDataGenerator(#preprocessing_function = self.attention_model.preprocess)\n",
    "                                          featurewise_center=True, \n",
    "                                          featurewise_std_normalization=True\n",
    "    #                                       rotation_range = 20,\n",
    "    #                                       width_shift_range = 0.2,\n",
    "    #                                       height_shift_range = 0.2,\n",
    "    #                                       shear_range = 0.2,\n",
    "    #                                       horizontal_flip = True,\n",
    "    #                                       vertical_flip = True,\n",
    "    #                                       preprocessing_function = ModelTrainer.img_normalize)\n",
    "    #                                       self.attention_model.preprocess)\n",
    "    #                                       fill_mode = 'nearest'\n",
    "                                               )\n",
    "        test_datagen = ImageDataGenerator(#preprocessing_function = self.attention_model.preprocess)\n",
    "                                        featurewise_center=True, \n",
    "                                        featurewise_std_normalization=True\n",
    "                                             )\n",
    "        if self.attention_model:\n",
    "            attention_generator = ImageDataGenerator(preprocessing_function = self.attention_model.heatmap)\n",
    "        else:\n",
    "            attention_generator = None\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(directory=self.train_dir, class_mode='sparse',shuffle=True,\n",
    "                                                            batch_size=self.batch_size,target_size=self.target_size)\n",
    "        validation_generator = test_datagen.flow_from_directory(directory=self.test_dir, class_mode='sparse',shuffle=False,\n",
    "                                                                batch_size=self.batch_size,target_size=self.target_size)\n",
    "        return train_generator, validation_generator, attention_generator\n",
    "\n",
    "\n",
    "    def create_class_weight(self, labels_dict, mu):\n",
    "        '''\n",
    "        Adjusts the class weights based on thr imbalance in the class\n",
    "        :param labels_dict: one hot encoding of labesl dictionary\n",
    "        :param mu: hyperparamter to adjust the setting of class weights\n",
    "        '''\n",
    "        total = np.sum(list(labels_dict.values()))\n",
    "        keys = labels_dict.keys()\n",
    "        class_weight = dict()\n",
    "\n",
    "        for key in keys:\n",
    "            score = math.log(mu*total/float(labels_dict[key]))\n",
    "            score = mu*total/float(labels_dict[key])\n",
    "            class_weight[key] = score if score > 1.0 else 1.0\n",
    "        return class_weight\n",
    "\n",
    "    def set_class_weight(self, mu):  \n",
    "        '''\n",
    "        Sets the weights for each class in the data. Weight can be adjusted to favour one class over other.\n",
    "        :param mu: hyperparamter to adjust the setting of class weights\n",
    "        '''\n",
    "        class_dict = dict()\n",
    "        for dir in os.listdir(self.train_dir):\n",
    "            class_dict[dir] = len(os.listdir(f\"{self.train_dir}/{dir}\"))\n",
    "\n",
    "        weights = self.create_class_weight(class_dict, mu)\n",
    "        self.class_weight = {}\n",
    "        class_indices = self.train_generator.class_indices\n",
    "        for cls in weights:\n",
    "            self.class_weight[class_indices[cls]] = weights[cls]\n",
    "\n",
    "\n",
    "    def _callback(self):\n",
    "        '''\n",
    "        Callback method for each epoch. Saves the model if the loss for the epoch is better than the previous epochs\n",
    "        '''\n",
    "        filepath = self.checkpoint_path + '/weights.h5'\n",
    "        checkpoint_dir = os.path.dirname(self.checkpoint_path)\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
    "                                                        save_weights_only=True,\n",
    "                                                        verbose=2,\n",
    "                                                        save_best_only=True)\n",
    "        learning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n",
    "                                                patience=3, \n",
    "                                                verbose=1, \n",
    "                                                factor=0.5, \n",
    "                                                min_lr=0.00001)\n",
    "        return [cp_callback,learning_rate_reduction]\n",
    "\n",
    "    def model_architecture(self, model_params, CLASS_N = 3):\n",
    "        '''\n",
    "        If a keras supported pretrained model is used, it creates a model based on the that.\n",
    "        Other model creating parameters are passed in the yaml config\n",
    "        '''\n",
    "        arch = eval(model_params.arch)\n",
    "        pretrained = arch(input_shape = (224,224,3), include_top=False, weights=model_params.pretrained_weight or None)\n",
    "\n",
    "        for layer in pretrained.layers:\n",
    "            layer.trainable = not(model_params.freeze_pretrained)\n",
    "\n",
    "        x = Flatten()(pretrained.layers[-1].output)\n",
    "        #x = Dense(5000, kernel_regularizer=regularizers.l1_l2(0.00001), activity_regularizer=regularizers.l2(0.00001), activation='relu',kernel_initializer=tf.keras.initializers.he_normal())(x) \n",
    "        #x = Dropout(0.5)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(3, activation = 'softmax')(x)\n",
    "\n",
    "        model = Model(inputs = pretrained.input, outputs = x)\n",
    "        print(\"New model created\")\n",
    "        return model\n",
    "\n",
    "    def load_trained_model(self):\n",
    "        '''\n",
    "        If a trained model exists in the base directory, it will be loaded from there when this function is called.\n",
    "        '''\n",
    "        if os.path.exists(self.checkpoint_path):\n",
    "            print(\"Trained model exists and it will be loaded\")\n",
    "            self.model = load_model(f'{self.checkpoint_path}/model.h5')\n",
    "            self.model.load_weights(f'{self.checkpoint_path}/weights.h5')\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def train(self, epochs=10, verbose=2):\n",
    "        '''\n",
    "        Model training\n",
    "        '''\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            model_info = self.model.fit(\n",
    "                                        x=self.train_generator, \n",
    "                                        steps_per_epoch=self.train_generator.samples // self.batch_size+1,  \n",
    "                                        epochs=epochs, \n",
    "                                        validation_steps=self.validation_generator.samples // self.batch_size+1,\n",
    "                                        validation_data=self.validation_generator, \n",
    "                                        verbose=verbose,\n",
    "                                        callbacks=self._callback(),\n",
    "                                        class_weight=self.class_weight\n",
    "                                        )\n",
    "            self.model_info = model_info\n",
    "            with open(f'{self.model_dir}/history.json','w') as fp:\n",
    "                json.dump(str(self.model_info.history), fp)\n",
    "\n",
    "    def confusion_matrix(self):\n",
    "        '''\n",
    "        Generates confusion matrix for the trained model.\n",
    "        '''\n",
    "        Y_pred = self.model.predict(self.validation_generator, self.validation_generator.samples // self.batch_size+1)\n",
    "        y_pred = np.argmax(Y_pred, axis=1)\n",
    "        cm = confusion_matrix(self.validation_generator.classes, y_pred)\n",
    "        target_names = list(model_trainer.validation_generator.class_indices.keys())\n",
    "        cls_rpt = classification_report(self.validation_generator.classes, y_pred, target_names=target_names)\n",
    "        #self.plot_confusion_matrix(cm, target_names)\n",
    "\n",
    "    def plot_confusion_matrix(self):\n",
    "        '''\n",
    "        Given an image as an input, this function predicts the label of the image based on the trained model.\n",
    "        '''\n",
    "        normalize=False\n",
    "        title='Confusion matrix'\n",
    "        cmap=plt.cm.Blues\n",
    "\n",
    "        Y_pred = self.model.predict(self.validation_generator, self.validation_generator.samples // self.batch_size+1)\n",
    "        y_pred = np.argmax(Y_pred, axis=1)\n",
    "        cm = confusion_matrix(self.validation_generator.classes, y_pred)\n",
    "        classes = list(model_trainer.validation_generator.class_indices.keys())\n",
    "\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                        horizontalalignment=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')  \n",
    "\n",
    "        cls_rpt = classification_report(self.validation_generator.classes, y_pred, target_names=classes)\n",
    "        print(cls_rpt)\n",
    "\n",
    "    def display_training_curves(training, validation, title, subplot):\n",
    "        ax = plt.subplot(subplot)\n",
    "        ax.plot(training)\n",
    "        ax.plot(validation)\n",
    "        ax.set_title('modelhttp://localhost:8888/notebooks/notebooks/Alol/31May_DATA5703_Alol.ipynb'+ title)\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend(['training', 'validation'])\n",
    "\n",
    "    def show_plot():\n",
    "        plt.subplots(figsize=(10,10))\n",
    "        plt.tight_layout()\n",
    "        display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "        display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10k_ResNet50_600450/weights.h5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base and ResNet50 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code modified from: https://www.tensorflow.org/tutorials/images/cnn\n",
    "#base function\n",
    "def ham_10000_base(CLASS_N=3):\n",
    "    model = keras.Sequential() # Sequential model type allows to build CNN model layer by layer\n",
    "    model.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32,32,3))) #convolution layer seen as 2D matrices, #activation function is rectified linear activation directly outputs input if its pos value\n",
    "    model.add(layers.MaxPooling2D((2, 2))) #dimensionality reduction computing maximum value in each window of 2x2\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten()) #connects Conv2D and dense layers\n",
    "    model.add(layers.Dense(64, activation='softmax')) #output layer, changed to softmax activation function so output can be interpreted as a probability\n",
    "    model.add(layers.Dense(CLASS_N, activation = 'softmax')) #output layer\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#visit here for more info on parameters: https://keras.io/api/applications/resnet/#resnet50-function\n",
    "# ResNet50 function\n",
    "def ham_10000_resnet50(CLASS_N=3):\n",
    "    \n",
    "    #model pre-trained on imagenet dataset\n",
    "    resnet_base = ResNet50(input_shape = (512,512,3), weights = 'imagenet', include_top = False, pooling = 'avg') \n",
    "    \n",
    "    for layer in resnet_base.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    x = Flatten()(resnet_base.layers[-1].output)\n",
    "    x = Dense(512, activation = 'relu', kernel_regularizer = regularizers.l2(0.001))(x)#regulariser reduces overfitting\n",
    "    x = Dropout(0.5)(x) #50% change in the output of neuron made 0 # also reduces overfitting\n",
    "    x = Dense(512, activation = 'relu', kernel_regularizer = regularizers.l2(0.001))(x)#regulariser reduces overfitting\n",
    "    x = Dropout(0.5)(x) #50% change in the output of neuron made 0 # also reduces overfitting\n",
    "    x = Dense(CLASS_N, activation = 'softmax', kernel_regularizer = regularizers.l2(0.001))(x)\n",
    "    \n",
    "    model = Model(inputs = resnet_base.input, outputs = x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50 train run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default resnet layers only - normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention model\n",
      "Found 25500 images belonging to 3 classes.\n",
      "Found 2027 images belonging to 3 classes.\n",
      "Trained model exists and it will be loaded\n"
     ]
    }
   ],
   "source": [
    "name = 'ham_10000_resnet50_noextralayers'\n",
    "model_config3 = yaml.safe_load(yaml_config3)\n",
    "model_trainer = ModelTrainer.load_from_config(model_config3[name], '/home/alol_elba/download/anaconda3/Capstone', \n",
    "                                               retrain = True)\n",
    "model_trainer.class_weight = {0:2.0,1:2.0,2:1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100352)       0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 100352)       401408      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            301059      batch_normalization[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 24,290,179\n",
      "Trainable params: 501,763\n",
      "Non-trainable params: 23,788,416\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_trainer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/33\n",
      "797/797 [==============================] - 287s 346ms/step - loss: 1.2176 - accuracy: 0.6654 - val_loss: 0.7415 - val_accuracy: 0.6231\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.74152, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 2/33\n",
      "797/797 [==============================] - 238s 299ms/step - loss: 0.8584 - accuracy: 0.7564 - val_loss: 0.7243 - val_accuracy: 0.6507\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.74152 to 0.72434, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 3/33\n",
      "797/797 [==============================] - 237s 298ms/step - loss: 0.7732 - accuracy: 0.7817 - val_loss: 0.8398 - val_accuracy: 0.6295\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.72434\n",
      "Epoch 4/33\n",
      "797/797 [==============================] - 237s 297ms/step - loss: 0.7291 - accuracy: 0.7909 - val_loss: 0.7309 - val_accuracy: 0.6739\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.72434\n",
      "Epoch 5/33\n",
      "797/797 [==============================] - 237s 298ms/step - loss: 0.6817 - accuracy: 0.8056 - val_loss: 0.7056 - val_accuracy: 0.6616\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.72434 to 0.70564, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 6/33\n",
      "797/797 [==============================] - 237s 297ms/step - loss: 0.6634 - accuracy: 0.8105 - val_loss: 0.6613 - val_accuracy: 0.7079\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.70564 to 0.66129, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 7/33\n",
      "797/797 [==============================] - 236s 296ms/step - loss: 0.6257 - accuracy: 0.8270 - val_loss: 0.5823 - val_accuracy: 0.7597\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.66129 to 0.58225, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 8/33\n",
      "797/797 [==============================] - 237s 298ms/step - loss: 0.5978 - accuracy: 0.8327 - val_loss: 0.7155 - val_accuracy: 0.6769\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.58225\n",
      "Epoch 9/33\n",
      "797/797 [==============================] - 236s 295ms/step - loss: 0.5712 - accuracy: 0.8437 - val_loss: 0.6593 - val_accuracy: 0.7060\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.58225\n",
      "Epoch 10/33\n",
      "797/797 [==============================] - 235s 295ms/step - loss: 0.5536 - accuracy: 0.8489 - val_loss: 0.6321 - val_accuracy: 0.7287\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.58225\n",
      "Epoch 11/33\n",
      "797/797 [==============================] - 235s 295ms/step - loss: 0.5334 - accuracy: 0.8506 - val_loss: 0.7133 - val_accuracy: 0.6823\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.58225\n",
      "Epoch 12/33\n",
      "797/797 [==============================] - 235s 295ms/step - loss: 0.5140 - accuracy: 0.8602 - val_loss: 0.6726 - val_accuracy: 0.6852\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.58225\n",
      "Epoch 13/33\n",
      "797/797 [==============================] - 235s 294ms/step - loss: 0.4991 - accuracy: 0.8626 - val_loss: 0.5748 - val_accuracy: 0.7346\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.58225 to 0.57480, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 14/33\n",
      " 52/797 [>.............................] - ETA: 3:17 - loss: 0.4715 - accuracy: 0.8690"
     ]
    }
   ],
   "source": [
    "model_trainer.train(epochs=33, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bcc       0.61      0.74      0.67        95\n",
      "         mel       0.53      0.57      0.55       194\n",
      "      others       0.94      0.92      0.93      1738\n",
      "\n",
      "    accuracy                           0.88      2027\n",
      "   macro avg       0.69      0.74      0.71      2027\n",
      "weighted avg       0.88      0.88      0.88      2027\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEmCAYAAAAnRIjxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "model_trainer.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding extra layers - normalised (4 dense and dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention model\n",
      "Found 25500 images belonging to 3 classes.\n",
      "Found 2027 images belonging to 3 classes.\n",
      "New model created\n"
     ]
    }
   ],
   "source": [
    "name = 'ham_10000_resnet50_4D&D'\n",
    "model_config3 = yaml.safe_load(yaml_config3)\n",
    "model_trainer = ModelTrainer.load_from_config(model_config3[name], '/home/ubuntu/data/3classes', \n",
    "                                              retrain = True)\n",
    "\n",
    "model_trainer.class_weight = {0:2.0,1:2.0,2:1.0}\n",
    "\n",
    "model3 = ham_10000_resnet50()\n",
    "model_trainer.register_model(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          262656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            1539        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 25,163,651\n",
      "Trainable params: 1,575,939\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_trainer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "797/797 [==============================] - 299s 361ms/step - loss: 4.6750 - accuracy: 0.5660 - val_loss: 2.2728 - val_accuracy: 0.6912\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.27275, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 2/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 3.2450 - accuracy: 0.7773 - val_loss: 2.1239 - val_accuracy: 0.7193\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.27275 to 2.12392, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 3/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 2.8912 - accuracy: 0.8160 - val_loss: 1.9450 - val_accuracy: 0.7617\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.12392 to 1.94500, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 4/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 2.5782 - accuracy: 0.8414 - val_loss: 1.7622 - val_accuracy: 0.8037\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.94500 to 1.76217, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 5/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 2.3710 - accuracy: 0.8570 - val_loss: 1.6672 - val_accuracy: 0.7967\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.76217 to 1.66724, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 6/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 2.1232 - accuracy: 0.8760 - val_loss: 1.5797 - val_accuracy: 0.7953\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.66724 to 1.57970, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 7/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 1.9081 - accuracy: 0.8980 - val_loss: 1.4342 - val_accuracy: 0.8293\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.57970 to 1.43420, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 8/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 1.7668 - accuracy: 0.9039 - val_loss: 1.3425 - val_accuracy: 0.8461\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.43420 to 1.34246, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 9/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 1.6645 - accuracy: 0.9101 - val_loss: 1.3405 - val_accuracy: 0.8061\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.34246 to 1.34051, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 10/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 1.5024 - accuracy: 0.9199 - val_loss: 1.2701 - val_accuracy: 0.8293\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.34051 to 1.27014, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 11/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 1.3816 - accuracy: 0.9317 - val_loss: 1.2041 - val_accuracy: 0.8426\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.27014 to 1.20406, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 12/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 1.3159 - accuracy: 0.9333 - val_loss: 1.1638 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.20406 to 1.16376, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 13/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 1.2300 - accuracy: 0.9377 - val_loss: 1.0940 - val_accuracy: 0.8624\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.16376 to 1.09404, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 14/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 1.1475 - accuracy: 0.9423 - val_loss: 1.1461 - val_accuracy: 0.8421\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.09404\n",
      "Epoch 15/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 1.1039 - accuracy: 0.9472 - val_loss: 1.1001 - val_accuracy: 0.8387\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.09404\n",
      "Epoch 16/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 1.0291 - accuracy: 0.9478 - val_loss: 1.1081 - val_accuracy: 0.8377\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.09404\n",
      "Epoch 17/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.9787 - accuracy: 0.9522 - val_loss: 1.0361 - val_accuracy: 0.8377\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.09404 to 1.03612, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 18/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.9575 - accuracy: 0.9524 - val_loss: 1.0161 - val_accuracy: 0.8456\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.03612 to 1.01614, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 19/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 0.8990 - accuracy: 0.9584 - val_loss: 1.0456 - val_accuracy: 0.8328\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.01614\n",
      "Epoch 20/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.8646 - accuracy: 0.9592 - val_loss: 0.9870 - val_accuracy: 0.8451\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.01614 to 0.98701, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 21/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.8325 - accuracy: 0.9600 - val_loss: 0.9496 - val_accuracy: 0.8638\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.98701 to 0.94959, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 22/40\n",
      "797/797 [==============================] - 249s 313ms/step - loss: 0.8049 - accuracy: 0.9634 - val_loss: 0.9375 - val_accuracy: 0.8777\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.94959 to 0.93752, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 23/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 0.7840 - accuracy: 0.9646 - val_loss: 0.9214 - val_accuracy: 0.8604\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.93752 to 0.92138, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 24/40\n",
      "797/797 [==============================] - 253s 317ms/step - loss: 0.7587 - accuracy: 0.9645 - val_loss: 0.9276 - val_accuracy: 0.8663\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.92138\n",
      "Epoch 25/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.7249 - accuracy: 0.9681 - val_loss: 0.9107 - val_accuracy: 0.8673\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.92138 to 0.91067, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 26/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.7136 - accuracy: 0.9673 - val_loss: 0.9106 - val_accuracy: 0.8629\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.91067 to 0.91059, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 27/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.6962 - accuracy: 0.9689 - val_loss: 0.8959 - val_accuracy: 0.8698\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.91059 to 0.89589, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 28/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 0.6831 - accuracy: 0.9683 - val_loss: 0.8536 - val_accuracy: 0.8801\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.89589 to 0.85362, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 29/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 0.6636 - accuracy: 0.9712 - val_loss: 0.8626 - val_accuracy: 0.8604\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.85362\n",
      "Epoch 30/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.6619 - accuracy: 0.9676 - val_loss: 0.8654 - val_accuracy: 0.8599\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.85362\n",
      "Epoch 31/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.6355 - accuracy: 0.9716 - val_loss: 0.9168 - val_accuracy: 0.8594\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.85362\n",
      "Epoch 32/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.6165 - accuracy: 0.9729 - val_loss: 0.8374 - val_accuracy: 0.8742\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.85362 to 0.83735, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 33/40\n",
      "797/797 [==============================] - 249s 313ms/step - loss: 0.6089 - accuracy: 0.9723 - val_loss: 0.9338 - val_accuracy: 0.8510\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.83735\n",
      "Epoch 34/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.5890 - accuracy: 0.9738 - val_loss: 0.8766 - val_accuracy: 0.8520\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.83735\n",
      "Epoch 35/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.5902 - accuracy: 0.9750 - val_loss: 0.8704 - val_accuracy: 0.8574\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.83735\n",
      "Epoch 36/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.5805 - accuracy: 0.9739 - val_loss: 0.8504 - val_accuracy: 0.8619\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.83735\n",
      "Epoch 37/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.5701 - accuracy: 0.9764 - val_loss: 0.8727 - val_accuracy: 0.8633\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.83735\n",
      "Epoch 38/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.5714 - accuracy: 0.9739 - val_loss: 0.8156 - val_accuracy: 0.8693\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.83735 to 0.81560, saving model to /home/alol_elba/download/anaconda3/Capstone/dataset/models/HAM_10000_ResNet50/weights.h5\n",
      "Epoch 39/40\n",
      "797/797 [==============================] - 250s 313ms/step - loss: 0.5529 - accuracy: 0.9750 - val_loss: 0.8390 - val_accuracy: 0.8629\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.81560\n",
      "Epoch 40/40\n",
      "797/797 [==============================] - 250s 314ms/step - loss: 0.5558 - accuracy: 0.9745 - val_loss: 0.8863 - val_accuracy: 0.8515\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.81560\n"
     ]
    }
   ],
   "source": [
    "model_trainer.train(epochs=40, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bcc       0.52      0.79      0.62        95\n",
      "         mel       0.45      0.58      0.51       194\n",
      "      others       0.94      0.88      0.91      1738\n",
      "\n",
      "    accuracy                           0.85      2027\n",
      "   macro avg       0.64      0.75      0.68      2027\n",
      "weighted avg       0.88      0.85      0.86      2027\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "model_trainer.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW AWS CREATED Adding extra layers - normalised (3 dense and dropout) 600,450 res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention model\n",
      "Found 25500 images belonging to 3 classes.\n",
      "Found 2051 images belonging to 3 classes.\n",
      "New model created\n"
     ]
    }
   ],
   "source": [
    "name = 'ham_10000_resnet50_3D&D600450'\n",
    "model_config3 = yaml.safe_load(yaml_config3)\n",
    "model_trainer = ModelTrainer.load_from_config(model_config3[name], '/home/ubuntu/data/3classes', \n",
    "                                              retrain = True)\n",
    "\n",
    "model_trainer.class_weight = {0:2.0,1:2.0,2:1.0}\n",
    "\n",
    "model3 = ham_10000_resnet50()\n",
    "model_trainer.register_model(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 600, 450, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 606, 456, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 300, 225, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 300, 225, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 300, 225, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 302, 227, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 150, 113, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 150, 113, 64) 4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 150, 113, 64) 256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 150, 113, 64) 0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 150, 113, 64) 36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 150, 113, 64) 256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 150, 113, 64) 0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 150, 113, 256 16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 150, 113, 256 16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 150, 113, 256 1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 150, 113, 256 1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 150, 113, 256 0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 150, 113, 256 0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 150, 113, 64) 16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 150, 113, 64) 256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 150, 113, 64) 0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 150, 113, 64) 36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 150, 113, 64) 256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 150, 113, 64) 0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 150, 113, 256 16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 150, 113, 256 1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 150, 113, 256 0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 150, 113, 256 0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 150, 113, 64) 16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 150, 113, 64) 256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 150, 113, 64) 0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 150, 113, 64) 36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 150, 113, 64) 256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 150, 113, 64) 0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 150, 113, 256 16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 150, 113, 256 1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 150, 113, 256 0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 150, 113, 256 0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 75, 57, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 75, 57, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 75, 57, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 75, 57, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 75, 57, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 75, 57, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 75, 57, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 75, 57, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 75, 57, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 75, 57, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 75, 57, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 75, 57, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 75, 57, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 75, 57, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 75, 57, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 75, 57, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 75, 57, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 75, 57, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 75, 57, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 75, 57, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 75, 57, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 75, 57, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 75, 57, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 75, 57, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 75, 57, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 75, 57, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 75, 57, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 75, 57, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 75, 57, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 75, 57, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 75, 57, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 75, 57, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 75, 57, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 75, 57, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 75, 57, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 75, 57, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 75, 57, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 75, 57, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 75, 57, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 75, 57, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 75, 57, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 75, 57, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 38, 29, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 38, 29, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 38, 29, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 38, 29, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 38, 29, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 38, 29, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 38, 29, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 38, 29, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 38, 29, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 38, 29, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 38, 29, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 38, 29, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 38, 29, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 38, 29, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 38, 29, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 38, 29, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 38, 29, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 38, 29, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 38, 29, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 38, 29, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 38, 29, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 38, 29, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 38, 29, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 38, 29, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 38, 29, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 38, 29, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 38, 29, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 38, 29, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 38, 29, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 38, 29, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 38, 29, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 38, 29, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 38, 29, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 38, 29, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 38, 29, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 38, 29, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 38, 29, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 38, 29, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 38, 29, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 38, 29, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 38, 29, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 38, 29, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 38, 29, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 38, 29, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 38, 29, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 38, 29, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 38, 29, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 38, 29, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 38, 29, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 38, 29, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 38, 29, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 19, 15, 512)  524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 19, 15, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 19, 15, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 19, 15, 512)  2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 19, 15, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 19, 15, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 19, 15, 2048) 2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 19, 15, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 19, 15, 2048) 8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 19, 15, 2048) 8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 19, 15, 2048) 0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 19, 15, 2048) 0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 19, 15, 512)  1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 19, 15, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 19, 15, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 19, 15, 512)  2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 19, 15, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 19, 15, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 19, 15, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 19, 15, 2048) 8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 19, 15, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 19, 15, 2048) 0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 19, 15, 512)  1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 19, 15, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 19, 15, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 19, 15, 512)  2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 19, 15, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 19, 15, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 19, 15, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 19, 15, 2048) 8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 19, 15, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 19, 15, 2048) 0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            1539        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,900,995\n",
      "Trainable params: 1,313,283\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_trainer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/33\n",
      "797/797 [==============================] - 467s 577ms/step - loss: 2.4372 - accuracy: 0.6587 - val_loss: 1.5055 - val_accuracy: 0.7206\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.50545, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 2/33\n",
      "797/797 [==============================] - 459s 575ms/step - loss: 1.5741 - accuracy: 0.8294 - val_loss: 1.2058 - val_accuracy: 0.8147\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.50545 to 1.20585, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 3/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 1.3022 - accuracy: 0.8617 - val_loss: 1.0716 - val_accuracy: 0.8279\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.20585 to 1.07156, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 4/33\n",
      "797/797 [==============================] - 459s 575ms/step - loss: 1.1105 - accuracy: 0.8823 - val_loss: 0.9993 - val_accuracy: 0.8264\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.07156 to 0.99933, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 5/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 0.9987 - accuracy: 0.8862 - val_loss: 0.8692 - val_accuracy: 0.8562\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.99933 to 0.86917, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 6/33\n",
      "797/797 [==============================] - 459s 576ms/step - loss: 0.8927 - accuracy: 0.8961 - val_loss: 0.8229 - val_accuracy: 0.8450\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.86917 to 0.82288, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 7/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 0.8045 - accuracy: 0.9047 - val_loss: 0.7858 - val_accuracy: 0.8474\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.82288 to 0.78581, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 8/33\n",
      "797/797 [==============================] - 459s 576ms/step - loss: 0.7378 - accuracy: 0.9097 - val_loss: 0.6952 - val_accuracy: 0.8747\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.78581 to 0.69525, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 9/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 0.6764 - accuracy: 0.9163 - val_loss: 0.6707 - val_accuracy: 0.8713\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.69525 to 0.67075, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 10/33\n",
      "797/797 [==============================] - 459s 575ms/step - loss: 0.6244 - accuracy: 0.9222 - val_loss: 0.7147 - val_accuracy: 0.8206\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.67075\n",
      "Epoch 11/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 0.5923 - accuracy: 0.9206 - val_loss: 0.6365 - val_accuracy: 0.8567\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.67075 to 0.63650, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 12/33\n",
      "797/797 [==============================] - 459s 576ms/step - loss: 0.5543 - accuracy: 0.9271 - val_loss: 0.5774 - val_accuracy: 0.8762\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.63650 to 0.57743, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 13/33\n",
      "797/797 [==============================] - 458s 574ms/step - loss: 0.5426 - accuracy: 0.9231 - val_loss: 0.5631 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.57743 to 0.56311, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 14/33\n",
      "797/797 [==============================] - 459s 575ms/step - loss: 0.5042 - accuracy: 0.9274 - val_loss: 0.5667 - val_accuracy: 0.8649\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.56311\n",
      "Epoch 15/33\n",
      "797/797 [==============================] - 458s 574ms/step - loss: 0.4870 - accuracy: 0.9317 - val_loss: 0.5572 - val_accuracy: 0.8610\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.56311 to 0.55725, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 16/33\n",
      "797/797 [==============================] - 458s 575ms/step - loss: 0.4701 - accuracy: 0.9327 - val_loss: 0.5308 - val_accuracy: 0.8810\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.55725 to 0.53080, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 17/33\n",
      "797/797 [==============================] - 459s 575ms/step - loss: 0.4408 - accuracy: 0.9383 - val_loss: 0.5815 - val_accuracy: 0.8411\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.53080\n",
      "Epoch 18/33\n",
      "797/797 [==============================] - 458s 574ms/step - loss: 0.4387 - accuracy: 0.9346 - val_loss: 0.5014 - val_accuracy: 0.8820\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.53080 to 0.50139, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 19/33\n",
      "797/797 [==============================] - 459s 576ms/step - loss: 0.4241 - accuracy: 0.9372 - val_loss: 0.4971 - val_accuracy: 0.8830\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.50139 to 0.49709, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 20/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 0.4128 - accuracy: 0.9408 - val_loss: 0.4841 - val_accuracy: 0.8874\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.49709 to 0.48406, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 21/33\n",
      "797/797 [==============================] - 459s 576ms/step - loss: 0.4035 - accuracy: 0.9384 - val_loss: 0.4789 - val_accuracy: 0.8874\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.48406 to 0.47889, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 22/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 0.3927 - accuracy: 0.9409 - val_loss: 0.5578 - val_accuracy: 0.8337\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.47889\n",
      "Epoch 23/33\n",
      "797/797 [==============================] - 459s 576ms/step - loss: 0.3831 - accuracy: 0.9426 - val_loss: 0.4982 - val_accuracy: 0.8649\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47889\n",
      "Epoch 24/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 0.3715 - accuracy: 0.9450 - val_loss: 0.4672 - val_accuracy: 0.8805\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.47889 to 0.46718, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 25/33\n",
      "797/797 [==============================] - 459s 575ms/step - loss: 0.3715 - accuracy: 0.9446 - val_loss: 0.4712 - val_accuracy: 0.8864\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.46718\n",
      "Epoch 26/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 0.3677 - accuracy: 0.9445 - val_loss: 0.4704 - val_accuracy: 0.8786\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.46718\n",
      "Epoch 27/33\n",
      "797/797 [==============================] - 459s 575ms/step - loss: 0.3622 - accuracy: 0.9429 - val_loss: 0.4593 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.46718 to 0.45929, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 28/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 0.3498 - accuracy: 0.9462 - val_loss: 0.4741 - val_accuracy: 0.8708\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.45929\n",
      "Epoch 29/33\n",
      "797/797 [==============================] - 459s 576ms/step - loss: 0.3413 - accuracy: 0.9494 - val_loss: 0.4646 - val_accuracy: 0.8879\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.45929\n",
      "Epoch 30/33\n",
      "797/797 [==============================] - 457s 573ms/step - loss: 0.3554 - accuracy: 0.9438 - val_loss: 0.4574 - val_accuracy: 0.8825\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.45929 to 0.45738, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n",
      "Epoch 31/33\n",
      "797/797 [==============================] - 459s 575ms/step - loss: 0.3331 - accuracy: 0.9493 - val_loss: 0.4808 - val_accuracy: 0.8684\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.45738\n",
      "Epoch 32/33\n",
      "797/797 [==============================] - 458s 574ms/step - loss: 0.3434 - accuracy: 0.9472 - val_loss: 0.4899 - val_accuracy: 0.8630\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.45738\n",
      "Epoch 33/33\n",
      "797/797 [==============================] - 459s 575ms/step - loss: 0.3186 - accuracy: 0.9549 - val_loss: 0.4452 - val_accuracy: 0.8840\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.45738 to 0.44521, saving model to /home/ubuntu/data/3classes/models//ham_10000_resnet50_3D&D600450/weights.h5\n"
     ]
    }
   ],
   "source": [
    "model_trainer.train(epochs=33, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bcc       0.64      0.66      0.65        95\n",
      "         mel       0.64      0.62      0.63       239\n",
      "      others       0.93      0.93      0.93      1717\n",
      "\n",
      "    accuracy                           0.88      2051\n",
      "   macro avg       0.74      0.74      0.74      2051\n",
      "weighted avg       0.88      0.88      0.88      2051\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "model_trainer.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW AWS CREATED 2 Adding extra layers - normalised (3 d&d), 512 res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention model\n",
      "Found 25500 images belonging to 3 classes.\n",
      "Found 2051 images belonging to 3 classes.\n",
      "New model created\n"
     ]
    }
   ],
   "source": [
    "name = 'ham_10k_resnet50_512'\n",
    "model_config3 = yaml.safe_load(yaml_config3)\n",
    "model_trainer = ModelTrainer.load_from_config(model_config3[name], '/home/ubuntu/data/3classes', retrain = True)\n",
    "\n",
    "model_trainer.class_weight = {0:2.0,1:2.0,2:1.0}\n",
    "\n",
    "model3 = ham_10000_resnet50()\n",
    "model_trainer.register_model(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 518, 3)  0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 256, 256, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 256, 256, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 256, 256, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 258, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 128, 128, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 128, 128, 64) 4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 128, 128, 64) 0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 128, 128, 64) 36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 128, 128, 64) 0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 128, 128, 256 16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 128, 128, 256 0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 128, 128, 256 0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 128, 128, 64) 16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 128, 128, 64) 0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 128, 128, 64) 36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 128, 128, 64) 0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 128, 128, 256 0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 128, 128, 256 0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 128, 128, 64) 16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 128, 128, 64) 0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 128, 128, 64) 36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 128, 128, 64) 256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 128, 128, 64) 0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 128, 128, 256 16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 128, 128, 256 1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 128, 128, 256 0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 128, 128, 256 0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 64, 64, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 64, 64, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 64, 64, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 64, 64, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 64, 64, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 64, 64, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 64, 64, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 64, 64, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 64, 64, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 64, 64, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 64, 64, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 64, 64, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 64, 64, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 64, 64, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 64, 64, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 64, 64, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 64, 64, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 64, 64, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 64, 64, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 64, 64, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 64, 64, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 64, 64, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 64, 64, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 64, 64, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 64, 64, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 32, 32, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 32, 32, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 32, 32, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 32, 32, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 32, 32, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 32, 32, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 32, 32, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 32, 32, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 32, 32, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 32, 32, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 32, 32, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 32, 32, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 32, 32, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 32, 32, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 32, 32, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 32, 32, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 32, 32, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 32, 32, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 32, 32, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 32, 32, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 32, 32, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 32, 32, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 32, 32, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 32, 32, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 32, 32, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 32, 32, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 32, 32, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 32, 32, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 32, 32, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 32, 32, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 32, 32, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 16, 16, 512)  524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 16, 16, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 16, 16, 512)  2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 16, 16, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 16, 16, 2048) 2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 16, 16, 2048) 0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 16, 16, 2048) 0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 16, 16, 512)  1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 16, 16, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 16, 16, 512)  2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 16, 16, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 16, 16, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 16, 16, 2048) 0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 16, 16, 512)  1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 16, 16, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 16, 16, 512)  2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 16, 16, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 16, 16, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 16, 16, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 16, 16, 2048) 8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 16, 16, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 16, 16, 2048) 0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2048)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          1049088     flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 512)          262656      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 3)            1539        dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,900,995\n",
      "Trainable params: 1,313,283\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_trainer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "797/797 [==============================] - 428s 530ms/step - loss: 2.4782 - accuracy: 0.6563 - val_loss: 1.4830 - val_accuracy: 0.7669\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.48302, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 2/50\n",
      "797/797 [==============================] - 419s 525ms/step - loss: 1.6139 - accuracy: 0.8289 - val_loss: 1.2595 - val_accuracy: 0.8157\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.48302 to 1.25950, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 3/50\n",
      "797/797 [==============================] - 419s 525ms/step - loss: 1.3420 - accuracy: 0.8601 - val_loss: 1.0586 - val_accuracy: 0.8674\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.25950 to 1.05858, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 4/50\n",
      "797/797 [==============================] - 421s 528ms/step - loss: 1.1459 - accuracy: 0.8826 - val_loss: 0.9883 - val_accuracy: 0.8498\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.05858 to 0.98830, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 5/50\n",
      "797/797 [==============================] - 419s 525ms/step - loss: 1.0107 - accuracy: 0.8908 - val_loss: 0.8691 - val_accuracy: 0.8693\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.98830 to 0.86909, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 6/50\n",
      "797/797 [==============================] - 420s 527ms/step - loss: 0.8874 - accuracy: 0.9051 - val_loss: 0.8172 - val_accuracy: 0.8640\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.86909 to 0.81725, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 7/50\n",
      "797/797 [==============================] - 419s 525ms/step - loss: 0.8080 - accuracy: 0.9123 - val_loss: 0.7641 - val_accuracy: 0.8679\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.81725 to 0.76411, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 8/50\n",
      "797/797 [==============================] - 418s 525ms/step - loss: 0.7416 - accuracy: 0.9142 - val_loss: 0.7173 - val_accuracy: 0.8713\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.76411 to 0.71731, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 9/50\n",
      "797/797 [==============================] - 421s 527ms/step - loss: 0.6771 - accuracy: 0.9220 - val_loss: 0.6806 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.71731 to 0.68063, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 10/50\n",
      "797/797 [==============================] - 418s 524ms/step - loss: 0.6217 - accuracy: 0.9259 - val_loss: 0.6508 - val_accuracy: 0.8703\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.68063 to 0.65077, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 11/50\n",
      "797/797 [==============================] - 419s 526ms/step - loss: 0.5958 - accuracy: 0.9242 - val_loss: 0.6282 - val_accuracy: 0.8732\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.65077 to 0.62821, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 12/50\n",
      "797/797 [==============================] - 419s 525ms/step - loss: 0.5501 - accuracy: 0.9323 - val_loss: 0.5932 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.62821 to 0.59323, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 13/50\n",
      "797/797 [==============================] - 419s 525ms/step - loss: 0.5160 - accuracy: 0.9354 - val_loss: 0.5724 - val_accuracy: 0.8737\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.59323 to 0.57238, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 14/50\n",
      "797/797 [==============================] - 420s 527ms/step - loss: 0.4993 - accuracy: 0.9316 - val_loss: 0.5570 - val_accuracy: 0.8776\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.57238 to 0.55701, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 15/50\n",
      "797/797 [==============================] - 418s 525ms/step - loss: 0.4655 - accuracy: 0.9381 - val_loss: 0.5562 - val_accuracy: 0.8654\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.55701 to 0.55624, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 16/50\n",
      "797/797 [==============================] - 420s 526ms/step - loss: 0.4494 - accuracy: 0.9406 - val_loss: 0.5488 - val_accuracy: 0.8674\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.55624 to 0.54884, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 17/50\n",
      "797/797 [==============================] - 419s 526ms/step - loss: 0.4302 - accuracy: 0.9404 - val_loss: 0.5252 - val_accuracy: 0.8766\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.54884 to 0.52524, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 18/50\n",
      "797/797 [==============================] - 419s 525ms/step - loss: 0.4177 - accuracy: 0.9437 - val_loss: 0.5153 - val_accuracy: 0.8771\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.52524 to 0.51527, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 19/50\n",
      "797/797 [==============================] - 421s 528ms/step - loss: 0.4056 - accuracy: 0.9450 - val_loss: 0.5334 - val_accuracy: 0.8606\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51527\n",
      "Epoch 20/50\n",
      "797/797 [==============================] - 418s 525ms/step - loss: 0.3916 - accuracy: 0.9460 - val_loss: 0.4981 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.51527 to 0.49805, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 21/50\n",
      "797/797 [==============================] - 417s 523ms/step - loss: 0.3713 - accuracy: 0.9482 - val_loss: 0.4910 - val_accuracy: 0.8903\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.49805 to 0.49097, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 22/50\n",
      "797/797 [==============================] - 418s 524ms/step - loss: 0.3666 - accuracy: 0.9490 - val_loss: 0.4845 - val_accuracy: 0.8922\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.49097 to 0.48452, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 23/50\n",
      "797/797 [==============================] - 414s 519ms/step - loss: 0.3595 - accuracy: 0.9496 - val_loss: 0.4976 - val_accuracy: 0.8723\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48452\n",
      "Epoch 24/50\n",
      "797/797 [==============================] - 420s 527ms/step - loss: 0.3560 - accuracy: 0.9506 - val_loss: 0.4749 - val_accuracy: 0.8820\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.48452 to 0.47486, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 25/50\n",
      "797/797 [==============================] - 418s 525ms/step - loss: 0.3427 - accuracy: 0.9546 - val_loss: 0.4903 - val_accuracy: 0.8688\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47486\n",
      "Epoch 26/50\n",
      "797/797 [==============================] - 420s 526ms/step - loss: 0.3471 - accuracy: 0.9508 - val_loss: 0.4832 - val_accuracy: 0.8649\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.47486\n",
      "Epoch 27/50\n",
      "797/797 [==============================] - 420s 526ms/step - loss: 0.3360 - accuracy: 0.9528 - val_loss: 0.4727 - val_accuracy: 0.8810\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.47486 to 0.47269, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 28/50\n",
      "797/797 [==============================] - 419s 525ms/step - loss: 0.3292 - accuracy: 0.9552 - val_loss: 0.4742 - val_accuracy: 0.8830\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.47269\n",
      "Epoch 29/50\n",
      "797/797 [==============================] - 421s 527ms/step - loss: 0.3186 - accuracy: 0.9561 - val_loss: 0.4546 - val_accuracy: 0.8932\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.47269 to 0.45465, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 30/50\n",
      "797/797 [==============================] - 419s 525ms/step - loss: 0.3073 - accuracy: 0.9612 - val_loss: 0.4561 - val_accuracy: 0.8869\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.45465\n",
      "Epoch 31/50\n",
      "797/797 [==============================] - 419s 526ms/step - loss: 0.3092 - accuracy: 0.9588 - val_loss: 0.4820 - val_accuracy: 0.8830\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.45465\n",
      "Epoch 32/50\n",
      "797/797 [==============================] - 420s 526ms/step - loss: 0.3151 - accuracy: 0.9572 - val_loss: 0.4428 - val_accuracy: 0.8952\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.45465 to 0.44280, saving model to /home/ubuntu/data/3classes/models//HAM_10k_ResNet50_512/weights.h5\n",
      "Epoch 33/50\n",
      "797/797 [==============================] - 419s 525ms/step - loss: 0.2931 - accuracy: 0.9631 - val_loss: 0.4728 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.44280\n",
      "Epoch 34/50\n",
      "797/797 [==============================] - 421s 528ms/step - loss: 0.2972 - accuracy: 0.9624 - val_loss: 0.4603 - val_accuracy: 0.8927\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.44280\n",
      "Epoch 35/50\n",
      "797/797 [==============================] - 418s 525ms/step - loss: 0.2960 - accuracy: 0.9614 - val_loss: 0.4752 - val_accuracy: 0.8742\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.44280\n",
      "Epoch 36/50\n",
      "797/797 [==============================] - 419s 526ms/step - loss: 0.2800 - accuracy: 0.9653 - val_loss: 0.4532 - val_accuracy: 0.8859\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.44280\n",
      "Epoch 37/50\n",
      "121/797 [===>..........................] - ETA: 5:32 - loss: 0.2811 - accuracy: 0.9683"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3afc7a028e5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-4590cf51f66d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, verbose)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             model_info = self.model.fit(\n\u001b[0m\u001b[1;32m    178\u001b[0m                                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_trainer.train(epochs=33, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         bcc       0.75      0.56      0.64        95\n",
      "         mel       0.69      0.51      0.58       239\n",
      "      others       0.91      0.96      0.94      1717\n",
      "\n",
      "    accuracy                           0.89      2051\n",
      "   macro avg       0.78      0.67      0.72      2051\n",
      "weighted avg       0.88      0.89      0.88      2051\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#original training CM\n",
    "%matplotlib inline\n",
    "model_trainer.plot_confusion_matrix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
